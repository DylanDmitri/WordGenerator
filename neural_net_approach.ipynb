{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from random import choice, randint, shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "from string import ascii_lowercase\n",
    "\n",
    "alph = ascii_lowercase + ' '\n",
    "\n",
    "maxsize = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = set(list(w.strip() for w in open('real.txt')))\n",
    "\n",
    "def isreal(word):\n",
    "    return word in words\n",
    "\n",
    "twords = tuple(words)\n",
    "\n",
    "\n",
    "def realword() -> str:\n",
    "    return choice(twords)\n",
    "\n",
    "ff = open('fake.txt', 'w')\n",
    "\n",
    "for _ in range(len(twords)):\n",
    "    ff.write(''.join(choice(ascii_lowercase) for _ in range(len(realword())))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8811020851135254\n"
     ]
    }
   ],
   "source": [
    "# from time import time\n",
    "# \n",
    "# r = tuple(open('RandomWords/real.txt'))\n",
    "# f = tuple(open('RandomWords/fake.txt'))\n",
    "# \n",
    "# def splatify(word):\n",
    "#     return tuple(chain(*[[int(c==alph[i]) for i in range(len(alph))] for c in word]))\n",
    "# \n",
    "# start = time()\n",
    "# w = np.array([\n",
    "#                  splatify(\n",
    "#                      list((' '+word.strip()).rjust(maxsize))\n",
    "#                  ) \n",
    "#                  for word in r+f])\n",
    "# \n",
    "# \n",
    "# a = [1]*len(r) + [0]*len(f)\n",
    "# print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.516174077987671\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "r = tuple(open('real.txt'))\n",
    "f = tuple(open('fake.txt'))\n",
    "\n",
    "# @numba.jit\n",
    "def splatify(word):\n",
    "    word = list(' '+word.strip().rjust(maxsize))\n",
    "    t = np.zeros(27*len(word))\n",
    "    \n",
    "    for i,char in enumerate(word):\n",
    "        if char not in alph:\n",
    "            print(word)\n",
    "        t[i*27 + alph.index(char)] = 1\n",
    "    \n",
    "    return t\n",
    "    \n",
    "#     \n",
    "# for word in f:\n",
    "#     print(word)\n",
    "#     for char in word.strip():\n",
    "#         if char not in alph:\n",
    "#             print(char)\n",
    "\n",
    "start = time()\n",
    "w = np.array([\n",
    "                 splatify(word)\n",
    "                 for word in r+f])\n",
    "\n",
    "\n",
    "a = [1]*len(r) + [0]*len(f)\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " - 6s - loss: 0.1421 - acc: 0.9458\n",
      "Epoch 2/3\n",
      " - 5s - loss: 0.0683 - acc: 0.9747\n",
      "Epoch 3/3\n",
      " - 5s - loss: 0.0473 - acc: 0.9826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1212c2ac8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(250, activation='relu', input_dim=len(splatify('test'))),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "assert len(w) == len(a)\n",
    "\n",
    "model.fit(w, a, epochs=3, batch_size=100, verbose=2)\n",
    "\n",
    "# scores = model.evaluate(w, a)\n",
    "# print(f\"\\nacc: {scores[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x:str) -> float:\n",
    "    t = np.array(splatify(x))\n",
    "    t = t.reshape((1 , 513))\n",
    "    return model.predict(t)[0][0]\n",
    "\n",
    "real_set = set(wd.strip() for wd in r)\n",
    "def isreal(to_check:str) -> bool:\n",
    "    return to_check in real_set\n",
    "\n",
    "\n",
    "def nonsense(s:int, plow, pup) -> str:\n",
    "    while True:\n",
    "        word = ''.join(choice(ascii_lowercase) for _ in range(s))\n",
    "        if plow <= predict(word) <= pup and not isreal(word):\n",
    "            return word\n",
    "\n",
    "def nonsentence(p_lower, p_upper) -> str:\n",
    "    return ' '.join(nonsense(randint(3,7), p_lower, p_upper) for _ in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yxafno fwzh dfvs brzamq mgnoev jqkgqca ywqruwe jsab veelrl bsb\n",
      "cmcc eyb ywafiq trpw bfw qqrk rvkzlrk jcurc ckcd cmdazd\n",
      "gwci xvws uyaui lwydiiy vdq yyzmitt znzyqxj ogvvo iacjvzf qxm\n",
      "govv xhdolr nygy bxw otu pwqaey ccel bhxl xwlwii dzg\n",
      "jcjtu bgkbq bicdvuj fxtngs thwe nbtb xejmwrq tovbdgo cggp glztsuj\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(nonsentence(0, .2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cudpeus mipheg umed oud dashal onfpred ysoun sbass veacils boyed\n",
      "dricgs lrow ose muvian rehden gooish poom oaous ise ded\n",
      "oue eidee smoct topber dubsoid dium botin uubose ezeles mafs\n",
      "ead rogon oms ond tong taice eradock folurah colviy hogsic\n",
      "iostirg etusint toxsed lepdy baopan ped oraimea lnogley tule foitls\n",
      "moal ejaisey sedlan momly geins woss hilwerk doiks ivins ows\n",
      "iad erhwads homar ubrnins ilisk tolies hlampte snocera vowe ochafe\n"
     ]
    }
   ],
   "source": [
    "out:str = ''\n",
    "for _ in range(7):\n",
    "    out += '\\n'\n",
    "    out += nonsentence(.95, 1)\n",
    "    \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130000\n",
      "cide bsed chay bulp wawe boit tofs groo erak cunk duad bape jash wrom hims teus faly crok ruas jamo hult quts ghan stty rral asic tant dufs flld mian prok ghas foan tway calp jous sapy lomy ctar gaks elut fogl coth domb wovo grus vomb liat wino lelk megs dapt tock inge wism cuge maln fulk ched reep ough dain wifs kned rede cace cogy ppat aset utal frub llar coar pung oods couy adst luat anin trub toom afus qurp mure gass golo aray sifs tala durb smup wack esen bive samb rted brys mang scuk balf\n"
     ]
    }
   ],
   "source": [
    "ostr = []\n",
    "\n",
    "for _ in range(100):\n",
    "    word = nonsense(4, .95, 1)\n",
    "    ostr.append(word)\n",
    "    \n",
    "print(' '.join(ostr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00 lorem\n",
      "1.00 ipsum\n",
      "1.00 dolor\n",
      "1.00 sit\n",
      "1.00 amet\n",
      "1.00 consectetur\n",
      "1.00 adipiscing\n",
      "1.00 elit\n",
      "1.00 phasellus\n",
      "1.00 laoreet\n",
      "1.00 eget\n",
      "1.00 ante\n",
      "1.00 a\n",
      "1.00 fringilla\n",
      "1.00 fusce\n",
      "1.00 eros\n",
      "1.00 felis\n",
      "1.00 volutpat\n",
      "1.00 at\n",
      "1.00 varius\n",
      "1.00 sit\n",
      "1.00 amet\n",
      "1.00 consectetur\n",
      "1.00 id\n",
      "1.00 purus\n",
      "1.00 mauris\n",
      "1.00 scelerisque\n",
      "1.00 pharetra\n",
      "1.00 est\n",
      "1.00 vel\n",
      "1.00 mattis\n",
      "1.00 mi\n",
      "1.00 euismod\n",
      "1.00 a\n",
      "1.00 sed\n",
      "1.00 vestibulum\n",
      "1.00 sed\n",
      "1.00 sapien\n",
      "1.00 sollicitudin\n",
      "1.00 rutrum\n",
      "1.00 vivamus\n",
      "1.00 vitae\n",
      "1.00 augue\n",
      "1.00 nulla\n",
      "1.00 nullam\n",
      "1.00 at\n",
      "1.00 consectetur\n",
      "1.00 nisl\n",
      "1.00 in\n",
      "1.00 laoreet\n",
      "1.00 diam\n",
      "1.00 vitae\n",
      "1.00 luctus\n",
      "1.00 laoreet\n",
      "1.00 dolor\n",
      "1.00 metus\n",
      "1.00 sollicitudin\n",
      "1.00 quam\n",
      "1.00 et\n",
      "1.00 lobortis\n",
      "1.00 velit\n",
      "1.00 enim\n",
      "1.00 eu\n",
      "1.00 dolor\n",
      "1.00 vivamus\n",
      "1.00 ac\n",
      "1.00 odio\n",
      "1.00 dui\n",
      "1.00 aliquam\n",
      "1.00 pulvinar\n",
      "1.00 nisl\n",
      "1.00 urna\n",
      "1.00 non\n",
      "1.00 dictum\n",
      "1.00 odio\n",
      "1.00 dapibus\n",
      "1.00 a\n",
      "1.00 sed\n",
      "1.00 luctus\n",
      "1.00 dictum\n",
      "1.00 tortor\n",
      "1.00 imperdiet\n",
      "1.00 maximus\n"
     ]
    }
   ],
   "source": [
    "lorem = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus laoreet eget ante a fringilla. Fusce eros felis, volutpat at varius sit amet, consectetur id purus. Mauris scelerisque pharetra est, vel mattis mi euismod a. Sed vestibulum sed sapien sollicitudin rutrum. Vivamus vitae augue nulla. Nullam at consectetur nisl. In laoreet, diam vitae luctus laoreet, dolor metus sollicitudin quam, et lobortis velit enim eu dolor. Vivamus ac odio dui. Aliquam pulvinar nisl urna, non dictum odio dapibus a. Sed luctus dictum tortor imperdiet maximus.'''\n",
    "\n",
    "def l_clean(l_word:str) -> str:\n",
    "    return ''.join(l_char for l_char in l_word.lower() if l_char in ascii_lowercase)\n",
    "\n",
    "\n",
    "\n",
    "for l_word in lorem.split():\n",
    "    l_word = l_clean(l_word)\n",
    "    print(f'{predict(l_word):.2f} {l_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.156 uair\n",
      "0.923 dair\n",
      "0.918 fair\n",
      "0.705 nair\n",
      "0.755 zair\n",
      "0.997 swanton\n",
      "0.083 upb\n",
      "0.997 jiggly\n",
      "0.015 jigglypuff\n",
      "0.805 ganondorf\n",
      "0.996 douglas\n",
      "0.924 pichu\n",
      "0.081 pikachu\n",
      "0.982 melee\n",
      "0.995 mewtwoking\n",
      "1.000 teambeer\n",
      "0.924 smokejoints\n",
      "0.410 bigdoinks\n",
      "0.808 amish\n",
      "0.823 dwarf\n",
      "0.995 sean\n",
      "0.997 rosebaugh\n",
      "0.999 rosebagel\n",
      "0.998 rosenbaum\n"
     ]
    }
   ],
   "source": [
    "for mterm in (('uair', 'dair', 'fair', 'nair', 'zair', 'swanton', 'upb',\n",
    "               'jiggly', 'jigglypuff', 'ganondorf', 'douglas',\n",
    "               'pichu', 'pikachu', 'melee', 'mewtwoking',\n",
    "               'teambeer', 'smokejoints', 'bigdoinks', 'amish', 'dwarf',\n",
    "               'sean', 'rosebaugh', 'rosebagel', 'rosenbaum')):\n",
    "    print(f'{predict(mterm):.3f} {mterm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.049262762069702\n"
     ]
    }
   ],
   "source": [
    "for mterm in (('height', 'omnipointment', 'tumpus', 'scrungo', 'muncho',\n",
    "               'plumbus',)):\n",
    "    print(f'{predict(mterm):.3f} {mterm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.049262762069702\n"
     ]
    }
   ],
   "source": [
    "def make_change(coins, n):\n",
    "    stack = [1] + [0]*n\n",
    "    \n",
    "    for coin in coins:\n",
    "        for j in range(coin, n+1):\n",
    "            stack[j] += stack[j-coin]\n",
    "            \n",
    "    return stack[-1]\n",
    "\n",
    "start = time()\n",
    "make_change([12, 15, 17, 22, 31, 67, 99, 123, 135, 868], 999999)\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.049262762069702\n"
     ]
    }
   ],
   "source": [
    "@numba.jit\n",
    "def make_change(coins, n):\n",
    "    stack = [1] + [0]*n\n",
    "    \n",
    "    for coin in coins:\n",
    "        for j in range(coin, n+1):\n",
    "            stack[j] += stack[j-coin]\n",
    "            \n",
    "    return stack[-1]\n",
    "\n",
    "start = time()\n",
    "make_change([12, 15, 17, 22, 31, 67, 99, 123, 135, 868], 999999)\n",
    "print(time()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
